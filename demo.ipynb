{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bash script example for running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "python action_recognition.py \\\n",
    "    --video /home/ciis/Desktop/shitass.mp4 \\\n",
    "    --out-filename /home/ciis/Desktop/shitass_out2.mp4 \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --skeleton-config configs/skeleton/posec3d/ciis_10.py \\\n",
    "    --skeleton-stdet-checkpoint work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth \\\n",
    "    --action-score-thr 0.75 \\\n",
    "    --label-map-stdet data/skeleton/ciis_label_map.txt \\\n",
    "    --predict-stepsize 2 \\\n",
    "    --output-fps 4\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='CIIS sp-te_ac-re demo')\n",
    "    parser.add_argument(\n",
    "        '--video',\n",
    "        default='data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah.mp4',\n",
    "        help='video file/url')\n",
    "    parser.add_argument(\n",
    "        '--out-filename',\n",
    "        default='data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah_out.mp4',\n",
    "        help='output filename')\n",
    "    parser.add_argument(\n",
    "        '--det-config',\n",
    "        default='mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py',\n",
    "        help='human detection config file path (from mmdet)')\n",
    "    parser.add_argument(\n",
    "        '--det-checkpoint',\n",
    "        default=('http://download.openmmlab.com/mmdetection/v2.0/'\n",
    "                 'faster_rcnn/faster_rcnn_r50_fpn_2x_coco/'\n",
    "                 'faster_rcnn_r50_fpn_2x_coco_'\n",
    "                 'bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'),\n",
    "        help='human detection checkpoint file/url')\n",
    "    parser.add_argument(\n",
    "        '--det-score-thr',\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        help='the threshold of human detection score')\n",
    "    parser.add_argument(\n",
    "        '--pose-config',\n",
    "        default='mmaction2/demo/demo_configs'\n",
    "        '/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py',\n",
    "        help='human pose estimation config file path (from mmpose)')\n",
    "    parser.add_argument(\n",
    "        '--pose-checkpoint',\n",
    "        default=('https://download.openmmlab.com/mmpose/top_down/hrnet/'\n",
    "                 'hrnet_w32_coco_256x192-c78dce93_20200708.pth'),\n",
    "        help='human pose estimation checkpoint file/url')\n",
    "    parser.add_argument(\n",
    "        '--skeleton-config',\n",
    "        default='configs/skeleton/posec3d'\n",
    "        '/ciis_10.py',\n",
    "        help='skeleton-based action recognition config file path')\n",
    "    parser.add_argument(\n",
    "        '--skeleton-stdet-checkpoint',\n",
    "        default=('work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth'),\n",
    "        help='skeleton-based spatio temporal detection checkpoint file/url')\n",
    "    parser.add_argument(\n",
    "        '--action-score-thr',\n",
    "        type=float,\n",
    "        default=0.75,\n",
    "        help='the threshold of action prediction score')\n",
    "    parser.add_argument(\n",
    "        '--label-map-stdet',\n",
    "        default='data/skeleton/ciis_label_map.txt',\n",
    "        help='label map file for spatio-temporal action detection')\n",
    "    parser.add_argument(\n",
    "        '--predict-stepsize',\n",
    "        default=4,\n",
    "        type=int,\n",
    "        help='give out a spatio-temporal detection prediction per n frames')\n",
    "    parser.add_argument(\n",
    "        '--output-stepsize',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=('show one frame per n frames in the demo, we should have: '\n",
    "              'predict_stepsize % output_stepsize == 0'))\n",
    "    parser.add_argument(\n",
    "        '--output-fps',\n",
    "        default=12,\n",
    "        type=int,\n",
    "        help='the fps of demo video output')\n",
    "    parser.add_argument(\n",
    "        '--device', type=str, default='cuda:0', help='CPU/CUDA device option')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        default={},\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file. For example, '\n",
    "        \"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\")\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### argparse alter for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_parser:\n",
    "    def __init__(self):\n",
    "        self.video = '/home/ciis/Desktop/shitass.mp4'\n",
    "        self.out_filename = '/home/ciis/Desktop/shitass_out2.mp4'\n",
    "        self.det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "        self.det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "        self.det_score_thr = 0.9\n",
    "        self.pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "        self.pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "        self.skeleton_config = 'configs/skeleton/posec3d/ciis_10.py'\n",
    "        self.skeleton_stdet_checkpoint = 'work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth'\n",
    "        self.action_score_thr = 0.75\n",
    "        self.label_map_stdet = 'data/skeleton/ciis_label_map.txt'\n",
    "        self.predict_stepsize = 2\n",
    "        self.output_fps = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy as cp\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmengine import DictAction\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_recognizer,\n",
    "                           init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "# import torch\n",
    "import torchvision\n",
    "# import cv2\n",
    "import time\n",
    "import serial\n",
    "# import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "from pathlib import Path\n",
    "from boxmot import StrongSort\n",
    "\n",
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "\n",
    "THICKNESS = 2  # int\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0)  # Change to 'cuda' if you have a GPU available\n",
    "\n",
    "tracker = StrongSort(\n",
    "    reid_weights=Path('osnet_x0_25_msmt17.pt'),  # ReID model to use\n",
    "    device=device,\n",
    "    half=False,\n",
    ")\n",
    "# arduino = serial.Serial(port='/dev/ttyUSB0', baudrate=115200, timeout=1)\n",
    "\n",
    "def sendData(pan, tilt):\n",
    "    command = f\"DEG {pan},{tilt}\\n\"\n",
    "    # arduino.write(command.encode())\n",
    "    print(f\"Sent command: {command}\")\n",
    "\n",
    "# Function to generate a unique color for each track ID\n",
    "def get_color(track_id):\n",
    "    np.random.seed(int(track_id))\n",
    "    return tuple(np.random.randint(0, 255, 3).tolist())\n",
    "\n",
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizer/ annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "def visualize(args,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    if na == 0:\n",
    "        na+=1\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // na\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(args.pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=False,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=True,\n",
    "                draw_pred=True,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                track_id = ann[3]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text1 = abbrev(lb)\n",
    "                    text1 = ': '.join([text1, f'{(score[k]*100):.1f}%'])\n",
    "                    text2 = f'ID: {int(track_id)}'\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    location2 = (0 + st[0], 18 + k * 18 + st[1]-25)\n",
    "                    textsize = cv2.getTextSize(text1, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    bahaya = ['melempar', 'membidik senapan', 'membidik pistol', 'memukul', 'menendang', 'menusuk']\n",
    "                    FONTCOLOR = (255, 0, 0) if lb in bahaya else (255, 255, 255)\n",
    "                    cv2.putText(frame, text2, location2, FONTFACE, FONTSCALE,\n",
    "                                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "                    cv2.putText(frame, text1, location, FONTFACE, FONTSCALE,\n",
    "                                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (int -> label name).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w, track_id_list):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "        track_id_list (list[int]): The list of ID of the tracked object.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    human_detection[:, 0::2] /= img_w\n",
    "    human_detection[:, 1::2] /= img_h\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res, id in zip(human_detection, result, track_id_list):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1] for x in res], id))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POSEC3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_based_stdet(args, label_map, human_detections, pose_results,\n",
    "                         num_frame, clip_len, frame_interval, h, w, scele_config):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           args.predict_stepsize)\n",
    "\n",
    "    #skeleton_config = mmengine.Config.fromfile(args.skeleton_config)\n",
    "    skeleton_config = scele_config\n",
    "    num_class = max(label_map.keys()) + 1  # for CIIS dataset (9 + 1) == len(label_map)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class\n",
    "    print(\"test here\")\n",
    "    skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "                                           args.skeleton_stdet_checkpoint,\n",
    "                                           args.device)\n",
    "\n",
    "    skeleton_predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dict='',\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                origin_shape=(h, w),\n",
    "                start_index=0,\n",
    "                modality='Pose',\n",
    "                num_clips=1,\n",
    "                clip_len=clip_len,\n",
    "                total_frames=num_frame)\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]  #x1, y1, x2, y2\n",
    "            area = expand_bbox(person_bbox, h, w)\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):  # num_person\n",
    "                    iou = cal_iou(bbox, area)\n",
    "                    if max_iou < iou:  # if isBelong\n",
    "                        index = k\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # for multi-label recognition\n",
    "            score = output.pred_score.tolist()\n",
    "            for k in range(len(score)):  # 10\n",
    "                if k not in label_map:\n",
    "                    continue\n",
    "                if score[k] > args.action_score_thr:\n",
    "                    skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "\n",
    "            # crop the image -> resize -> extract pose -> as input for poseC3D\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MY code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_webcam(frame_rate = 4, frame_predict = 4):\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    vid = \"/home/ciis/Desktop/aldy_septi/30d_2s1.mp4\"\n",
    "    cap = cv2.VideoCapture(0)  # '0' if webcam, \"vid\" if video\n",
    "\n",
    "    while frame_count < frame_predict * frame_rate:\n",
    "        _, frame = cap.read()\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "    return frames\n",
    "\n",
    "def main():\n",
    "    print(\"AAAAA\")\n",
    "    # args = parse_args()\n",
    "    args = arg_parser()\n",
    "    model = YOLO('yolov8l.pt')  # Replace with your model path\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # Start capturing video from the webcam\n",
    "    # cap = cv2.VideoCapture(0)\n",
    "    ctime=0\n",
    "    ptime = 0\n",
    "\n",
    "    tmp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    frame_rate = 2\n",
    "    frame_predict = args.predict_stepsize\n",
    "    skele_config = mmengine.Config.fromfile(args.skeleton_config)\n",
    "\n",
    "    print(\"Press Q to stop\")\n",
    "\n",
    "    while True:\n",
    "        original_frames = capture_webcam(frame_rate, frame_predict)\n",
    "        first_frame = original_frames[0]\n",
    "        num_frame = len(original_frames)\n",
    "        print(num_frame)\n",
    "        h, w, _ = original_frames[0].shape\n",
    "        start = time.time()\n",
    "\n",
    "        # get Human detection results\n",
    "        print(\"test human detection\")\n",
    "\n",
    "        # processed_detections = human_detections \n",
    "        results = model(original_frames, classes=[0])  # Detect only people (class 0)\n",
    "        \n",
    "        # Tracking\n",
    "        conf_thres = 0.5\n",
    "        dets = []\n",
    "        for box in results[0].boxes.cpu().numpy():\n",
    "            if box.conf[0].astype(float).round(2) > conf_thres:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].astype(float).round(2)\n",
    "                conf = box.conf[0].astype(float).round(2)\n",
    "                cls = box.cls[0].astype(int)\n",
    "                print(x1, y1, x2, y2, \"conf:\",conf, \"cls:\", cls)\n",
    "                dets.append([x1, y1, x2, y2, conf, cls])\n",
    "\n",
    "        dets = np.array(dets)\n",
    "        tracking_frame = np.array(first_frame)\n",
    "\n",
    "        print(\"inside dets frame:\" + str(dets))\n",
    "        print(\"inside origial frame:\" + str(tracking_frame))\n",
    "        \n",
    "        target = tracker.update(dets, tracking_frame)\n",
    "        print(\"target is: \" + str(target) + str(len(target)))\n",
    "        if len(target) == 0:\n",
    "            continue\n",
    "        \n",
    "        target_id_list = [ int(x[4]) for x in target ]\n",
    "        print(\"our target id list: \" + str(target_id_list))\n",
    "\n",
    "        #frame, coor_dets = draw_bboxes(target, original_frames[0])\n",
    "        # targetTracking(coor_dets)\n",
    "        # print(frame.shape)\n",
    "\n",
    "        #end = time.time()\n",
    "        #cv2.putText(frame, f'FPS: {1/(end-start):.2f}', (10, 50), font, 1.5, (0, 255, 255), 4)\n",
    "        #cv2.imshow('frame', frame)\n",
    "\n",
    "\n",
    "        # POse estimation\n",
    "        human_detections = []\n",
    "        for frame_idx in range(len(original_frames)):  # Loop over frames\n",
    "            frame_detections = []  # Temporary list for storing frame detections\n",
    "\n",
    "            # Get YOLO detections for the current frame\n",
    "            frame_results = results[frame_idx].boxes  # Modify this if results are batched differently\n",
    "            for detection in frame_results:\n",
    "                x1, y1, x2, y2 = detection.xyxy[0].cpu().numpy()  # Bounding box coordinates\n",
    "                frame_detections.append([x1, y1, x2, y2])  # Append only bbox (no conf, cls)\n",
    "\n",
    "            if frame_detections:  # If detections exist for the frame\n",
    "                human_detections.append(np.array(frame_detections, dtype=np.float32))\n",
    "            else:  # No detections for this frame\n",
    "                human_detections.append(np.empty((0, 4), dtype=np.float32))\n",
    "                \n",
    "        print(\"===== HUMAN DETECTION =====\")\n",
    "        print(human_detections)\n",
    "        print(\"===========================\")\n",
    "        \n",
    "        # get Pose estimation results\n",
    "        pose_datasample = None\n",
    "        print(\"test pose detection\")\n",
    "        pose_results, pose_datasample = pose_inference(\n",
    "            args.pose_config,\n",
    "            args.pose_checkpoint,\n",
    "            original_frames,\n",
    "            human_detections,\n",
    "            device=args.device)\n",
    "\n",
    "        # resize frames to shortside 720\n",
    "        # new_w, new_h = mmcv.rescale_size((w, h), (720, np.Inf))\n",
    "        new_w, new_h = w, h\n",
    "        # frames = [mmcv.imresize(img, (new_w, new_h)) for img in original_frames]\n",
    "        frames = original_frames\n",
    "        w_ratio, h_ratio = new_w / w, new_h / h\n",
    "\n",
    "        # Load spatio-temporal detection label_map\n",
    "        stdet_label_map = load_label_map(args.label_map_stdet)\n",
    "\n",
    "        stdet_preds = None\n",
    "\n",
    "        print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "        # clip_len, frame_interval = 30, 1\n",
    "        clip_len, frame_interval = args.predict_stepsize, 1\n",
    "        timestamps, stdet_preds = skeleton_based_stdet(args, stdet_label_map,\n",
    "                                                        human_detections,\n",
    "                                                        pose_results, num_frame,\n",
    "                                                        clip_len,\n",
    "                                                        frame_interval, h, w, skele_config)\n",
    "        for i in range(len(human_detections)):\n",
    "            det = human_detections[i]\n",
    "            det[:, 0:4:2] *= w_ratio\n",
    "            det[:, 1:4:2] *= h_ratio\n",
    "            human_detections[i] = torch.from_numpy(det[:, :4]).to(args.device)\n",
    "\n",
    "        stdet_results = []\n",
    "        for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "            human_detection = human_detections[timestamp - 1]\n",
    "            stdet_results.append(\n",
    "                pack_result(human_detection, prediction, new_h, new_w, \n",
    "                            target_id_list)\n",
    "                            )\n",
    "\n",
    "        def dense_timestamps(timestamps, n):\n",
    "            \"\"\"Make it nx frames.\"\"\"\n",
    "            old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "            start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "            new_frame_inds = np.arange(\n",
    "                len(timestamps) * n) * old_frame_interval / n + start\n",
    "            return new_frame_inds.astype(np.int64)\n",
    "\n",
    "        dense_n = int(args.predict_stepsize / args.output_stepsize)\n",
    "        # output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "        print(timestamps)\n",
    "        output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "        frames = [\n",
    "            original_frames[timestamp - 1]\n",
    "            # cv2.imread(\"../../../Downloads/1280x720-white-solid-color-background.jpg\")\n",
    "            for timestamp in output_timestamps\n",
    "        ]   \n",
    "\n",
    "        pose_datasample = [\n",
    "            pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "        ]\n",
    "\n",
    "        print(\"what inside stdet:\", stdet_results)\n",
    "        vis_frames = visualize(args, frames, stdet_results, pose_datasample,\n",
    "                            None)\n",
    "        \n",
    "        end = time.time()\n",
    "        cv2.putText(vis_frames[0], f'FPS: {1/(end-start):.2f}', (10, 50), font, 1.5, (0, 255, 255), 4)\n",
    "        cv2.imshow(\"Webcam Feed\", vis_frames[0])\n",
    "\n",
    "        vid = mpy.ImageSequenceClip(vis_frames, fps=args.output_fps)\n",
    "        vid.write_videofile(args.out_filename)\n",
    "\n",
    "        tmp_dir.cleanup()\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
